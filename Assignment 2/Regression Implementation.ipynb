{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "91ccfa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "68178629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature data\n",
    "X_train = pd.read_csv('housing_X_train.csv', header=None).values.T\n",
    "X_test = pd.read_csv('housing_X_test.csv', header=None).values.T\n",
    "\n",
    "# Load target data\n",
    "y_train = pd.read_csv('housing_y_train.csv', header=None).values.flatten()\n",
    "y_test = pd.read_csv('housing_y_test.csv', header=None).values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8ad1ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "  \n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41155bf2",
   "metadata": {},
   "source": [
    "# Ridge regression algorithm using the closed form solution for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dfd5848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_closed_form(X, y, lambda_reg):\n",
    "    # Number of samples and features\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Add intercept term by appending a column of ones to X\n",
    "    X_b = np.hstack([np.ones((n_samples, 1)), X])\n",
    "    \n",
    "    # Create the regularization matrix\n",
    "    I = np.eye(n_features + 1)\n",
    "    I[0, 0] = 0  \n",
    "    \n",
    "    # Compute (X^T X + λI)\n",
    "    XtX = X_b.T @ X_b\n",
    "    XtX_plus_lambdaI = XtX + n_samples*lambda_reg * I\n",
    "    \n",
    "    # Compute X^T y\n",
    "    Xty = X_b.T @ y\n",
    "    \n",
    "    # Solve for beta\n",
    "    beta = np.linalg.solve(XtX_plus_lambdaI, Xty)\n",
    "    \n",
    "    # Extract intercept and coefficients\n",
    "    intercept = beta[0]\n",
    "    coefficients = beta[1:]\n",
    "    \n",
    "    return intercept, coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "739e6db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and Testing Errors for Different λ Values using Ridge Regression closed-form solution:\n",
      "\n",
      "λ     Train MSE       Test MSE       \n",
      "-----------------------------------\n",
      "0     9.69            370.22         \n",
      "0.25  15.01           65.42          \n",
      "0.5   18.42           48.02          \n",
      "0.75  20.42           42.22          \n",
      "1     21.76           39.89          \n"
     ]
    }
   ],
   "source": [
    "# Define the list of lambda (λ) values to test\n",
    "lambdas = [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "print(\"\\nTraining and Testing Errors for Different λ Values using Ridge Regression closed-form solution:\\n\")\n",
    "print(f\"{'λ':<5} {'Train MSE':<15} {'Test MSE':<15}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for lambda_reg in lambdas:\n",
    "    # Train the model using Ridge Regression closed-form solution\n",
    "    intercept, coefficients = ridge_regression_closed_form(X_train, y_train, lambda_reg)\n",
    "    \n",
    "    # Make predictions on the training set\n",
    "    y_train_pred = X_train @ coefficients + intercept\n",
    "    \n",
    "    # Make predictions on the testing set\n",
    "    y_test_pred = X_test @ coefficients + intercept\n",
    "    \n",
    "    # Compute MSE for training and testing sets\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'Lambda': lambda_reg,\n",
    "        'Train MSE': train_mse,\n",
    "        'Test MSE': test_mse\n",
    "    })\n",
    "    \n",
    "    \n",
    "    print(f\"{lambda_reg:<5} {train_mse:<15.2f} {test_mse:<15.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "27b0c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation from the training data\n",
    "feature_means = np.mean(X_train, axis=0)\n",
    "feature_stds = np.std(X_train, axis=0)\n",
    "\n",
    "# Avoid division by zero by setting std to 1 where std is 0\n",
    "feature_stds[feature_stds == 0] = 1\n",
    "\n",
    "# Standardize training data\n",
    "X_train_scaled = (X_train - feature_means) / feature_stds\n",
    "\n",
    "# Standardize testing data using training data statistics\n",
    "X_test_scaled = (X_test - feature_means) / feature_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e195c65",
   "metadata": {},
   "source": [
    "# Ridge regression algorithm using the Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "11874b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_gradient_descent(X, y, lambda_reg, learning_rate, n_iterations):\n",
    "    # Number of samples and features\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize coefficients (including intercept)\n",
    "    beta = np.zeros(n_features + 1)\n",
    "    \n",
    "    # Add intercept term by appending a column of ones to X\n",
    "    X_b = np.hstack([np.ones((n_samples, 1)), X])\n",
    "    \n",
    "    # Gradient Descent\n",
    "    for iteration in range(n_iterations):\n",
    "        \n",
    "        y_pred = X_b @ beta\n",
    "        \n",
    "        # Gradient of the loss function\n",
    "        error = y_pred - y\n",
    "        gradient = (2 / n_samples) * (X_b.T @ error) + 2 * lambda_reg * np.r_[0, beta[1:]] \n",
    "        \n",
    "        # Update coefficients\n",
    "        beta -= learning_rate * gradient\n",
    "    \n",
    "    # Extract intercept and coefficients\n",
    "    intercept = beta[0]\n",
    "    coefficients = beta[1:]\n",
    "    \n",
    "    return intercept, coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "764d9910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and Testing Errors for Different λ Values using Ridge Regression with gradient descent:\n",
      "\n",
      "λ     Train MSE       Test MSE       \n",
      "-----------------------------------\n",
      "0     9.70            338.91         \n",
      "0.25  11.79           85.32          \n",
      "0.5   14.58           53.42          \n",
      "0.75  17.38           44.47          \n",
      "1     20.06           41.20          \n"
     ]
    }
   ],
   "source": [
    "# Set learning rate and number of iterations\n",
    "learning_rate= 0.01  \n",
    "n_iterations = 1000  \n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "print(\"\\nTraining and Testing Errors for Different λ Values using Ridge Regression with gradient descent:\\n\")\n",
    "print(f\"{'λ':<5} {'Train MSE':<15} {'Test MSE':<15}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for lambda_reg in lambdas:\n",
    "    # Train the model using Ridge Regression with gradient descent\n",
    "    intercept, coefficients = ridge_regression_gradient_descent(X_train_scaled, y_train, lambda_reg, learning_rate, n_iterations)\n",
    "    \n",
    "    # Make predictions on the training set\n",
    "    y_train_pred = X_train_scaled @ coefficients + intercept\n",
    "    \n",
    "    # Make predictions on the testing set\n",
    "    y_test_pred = X_test_scaled @ coefficients + intercept\n",
    "    \n",
    "    # Compute MSE for training and testing sets\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'Lambda': lambda_reg,\n",
    "        'Train MSE': train_mse,\n",
    "        'Test MSE': test_mse\n",
    "    })\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"{lambda_reg:<5} {train_mse:<15.2f} {test_mse:<15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd7ca5",
   "metadata": {},
   "source": [
    "# Lasso regression algorithm using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2759ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression_gradient_descent(X, y, lambda_reg, learning_rate, n_iterations):\n",
    "    # Number of samples and features\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize coefficients (including intercept)\n",
    "    beta = np.zeros(n_features + 1)\n",
    "    \n",
    "    # Add intercept term by appending a column of ones to X\n",
    "    X_b = np.hstack([np.ones((n_samples, 1)), X])\n",
    "    \n",
    "    # Gradient Descent with Sub-Gradient for L1 Regularization\n",
    "    for iteration in range(n_iterations):\n",
    "        # Predictions: y_pred = X_b @ beta\n",
    "        y_pred = X_b @ beta\n",
    "        \n",
    "        # Compute error\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Gradient of the loss function (Mean Squared Error)\n",
    "        gradient_loss = (2 / n_samples) * (X_b.T @ error)\n",
    "        \n",
    "        # Compute sub-gradient for L1 regularization\n",
    "        #np.sign(0) returns 0, which is a valid sub-gradient\n",
    "        sub_gradient = lambda_reg * np.sign(beta)\n",
    "        sub_gradient[0] = 0  \n",
    "        \n",
    "        # Total gradient\n",
    "        gradient = gradient_loss + sub_gradient\n",
    "        \n",
    "        # Update coefficients\n",
    "        beta -= learning_rate * gradient\n",
    "        \n",
    "    # Extract intercept and coefficients\n",
    "    intercept = beta[0]\n",
    "    coefficients = beta[1:]\n",
    "    \n",
    "    return intercept, coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "84a671f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and Testing Errors for Different λ Values using Lasso Regression with gradient descent:\n",
      "\n",
      "λ     Train MSE       Test MSE       \n",
      "-----------------------------------\n",
      "0     9.70            338.91         \n",
      "0.25  10.10           67.10          \n",
      "0.5   10.57           64.61          \n",
      "0.75  11.28           64.83          \n",
      "1     12.12           65.53          \n"
     ]
    }
   ],
   "source": [
    "# Set learning rate and number of iterations\n",
    "learning_rate= 0.01  \n",
    "n_iterations = 1000 \n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "print(\"\\nTraining and Testing Errors for Different λ Values using Lasso Regression with gradient descent:\\n\")\n",
    "print(f\"{'λ':<5} {'Train MSE':<15} {'Test MSE':<15}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for lambda_reg in lambdas:\n",
    "    # Train the model using Lasso Regression with gradient descent\n",
    "    intercept, coefficients = lasso_regression_gradient_descent(X_train_scaled, y_train, lambda_reg, learning_rate, n_iterations)\n",
    "    \n",
    "    # Make predictions on the training set\n",
    "    y_train_pred = X_train_scaled @ coefficients + intercept\n",
    "    \n",
    "    # Make predictions on the testing set\n",
    "    y_test_pred = X_test_scaled @ coefficients + intercept\n",
    "    \n",
    "    # Compute MSE for training and testing sets\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'Lambda': lambda_reg,\n",
    "        'Train MSE': train_mse,\n",
    "        'Test MSE': test_mse\n",
    "    })\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"{lambda_reg:<5} {train_mse:<15.2f} {test_mse:<15.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ddf1f26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression using Gradient Descent Coefficients:\n",
      " [-0.01119849  0.3946084  -0.44069534  0.25901836 -0.14501965  2.91693341\n",
      " -0.31266908 -0.62279104  0.16431131 -0.64867437 -1.16269428  0.4164327\n",
      " -1.6043875 ]\n",
      "Lasso Regression using Descent Coefficients:\n",
      " [-1.44517759e-03 -6.13581190e-03 -5.82747042e-03  4.27393042e-03\n",
      " -6.37042393e-03  6.32281854e+00 -4.16889426e-02 -6.47756942e-03\n",
      "  3.50778507e-03 -4.95216935e-01 -1.09186763e+00  9.91342616e-02\n",
      " -8.50224303e-01]\n"
     ]
    }
   ],
   "source": [
    "# Ridge Gradient Descent\n",
    "intercept_gd, coefficients_gd = ridge_regression_gradient_descent(X_train_scaled, y_train, lambda_reg, learning_rate, n_iterations)\n",
    "#Lasso Gradient Descent\n",
    "intercept_l, coefficients_l = lasso_regression_gradient_descent(X_train_scaled, y_train, lambda_reg, learning_rate, n_iterations)\n",
    "\n",
    "print(\"Ridge Regression using Gradient Descent Coefficients:\\n\", coefficients_gd)\n",
    "\n",
    "print(\"Lasso Regression using Descent Coefficients:\\n\", coefficients_l)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
