{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee79981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9f4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(file_path):\n",
    "    \"\"\"\n",
    "    Load MNIST images from the IDX file format.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: str, path to the idx3-ubyte file\n",
    "    \n",
    "    Returns:\n",
    "    - images: numpy.ndarray, shape (num_images, 28*28)\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the magic number and dimensions\n",
    "        magic, num_images, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError(f'Invalid magic number {magic} in image file: {file_path}')\n",
    "        \n",
    "        # Read the image data\n",
    "        image_data = f.read()\n",
    "        images = np.frombuffer(image_data, dtype=np.uint8)\n",
    "        images = images.reshape(num_images, rows * cols)\n",
    "        return images\n",
    "\n",
    "def load_labels(file_path):\n",
    "    \"\"\"\n",
    "    Load MNIST labels from the IDX file format.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: str, path to the idx1-ubyte file\n",
    "    \n",
    "    Returns:\n",
    "    - labels: numpy.ndarray, shape (num_labels,)\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the magic number and number of labels\n",
    "        magic, num_labels = struct.unpack('>II', f.read(8))\n",
    "        if magic != 2049:\n",
    "            raise ValueError(f'Invalid magic number {magic} in label file: {file_path}')\n",
    "        \n",
    "        # Read the label data\n",
    "        label_data = f.read()\n",
    "        labels = np.frombuffer(label_data, dtype=np.uint8)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f40a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your MNIST files\n",
    "train_images_path = 'data/train-images.idx3-ubyte'\n",
    "train_labels_path = 'data/train-labels.idx1-ubyte'\n",
    "test_images_path = 'data/t10k-images.idx3-ubyte'\n",
    "test_labels_path = 'data/t10k-labels.idx1-ubyte'\n",
    "\n",
    "# Load the data\n",
    "X_train = load_images(train_images_path)\n",
    "y_train = load_labels(train_labels_path)\n",
    "X_test = load_images(test_images_path)\n",
    "y_test = load_labels(test_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83737d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train.astype(np.float64) / 255.0\n",
    "X_test = X_test.astype(np.float64) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c291b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce dimensionality to 20\n",
    "pca = PCA(n_components=20, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d86ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM algorithm for cGMM\n",
    "def em_cgmm(X, K, max_iters=100, tol=1e-5):\n",
    "    num_samples, num_features = X.shape\n",
    "    cGMM_params = []  # List to store parameters for each feature\n",
    "\n",
    "    # Iterate over each feature dimension\n",
    "    for j in range(num_features):\n",
    "        # Extract the j-th feature column\n",
    "        X_j = X[:, j]\n",
    "\n",
    "        # Initialize parameters for the GMM of feature j\n",
    "        # Initialize means by randomly selecting K data points\n",
    "        indices = np.random.choice(num_samples, K, replace=False)\n",
    "        means = X_j[indices]\n",
    "        # Initialize variances to the variance of the data\n",
    "        variances = np.full(K, np.var(X_j) + 1e-6)\n",
    "        # Initialize weights equally\n",
    "        weights = np.full(K, 1 / K)\n",
    "        log_likelihood_old = None\n",
    "\n",
    "        # EM iterations\n",
    "        for iteration in range(max_iters):\n",
    "            # E-step: Compute responsibilities\n",
    "            # Compute Gaussian densities\n",
    "            densities = np.zeros((num_samples, K))\n",
    "            for k in range(K):\n",
    "                coef = 1 / np.sqrt(2 * np.pi * variances[k])\n",
    "                exponent = -0.5 * ((X_j - means[k]) ** 2) / variances[k]\n",
    "                densities[:, k] = coef * np.exp(exponent)\n",
    "\n",
    "            # Compute weighted densities\n",
    "            weighted_densities = densities * weights\n",
    "            total_density = np.sum(weighted_densities, axis=1, keepdims=True)\n",
    "            responsibilities = weighted_densities / (total_density + 1e-300)\n",
    "\n",
    "            # M-step: Update parameters\n",
    "            Nk = np.sum(responsibilities, axis=0)\n",
    "            weights = Nk / num_samples\n",
    "            means = np.sum(responsibilities * X_j[:, np.newaxis], axis=0) / Nk\n",
    "            variances = np.sum(responsibilities * (X_j[:, np.newaxis] - means) ** 2, axis=0) / Nk\n",
    "            variances[variances < 1e-6] = 1e-6  # Prevent variances from becoming too small\n",
    "\n",
    "            # Compute log-likelihood\n",
    "            log_likelihood = np.sum(np.log(total_density + 1e-300))\n",
    "            if log_likelihood_old is not None and np.abs(log_likelihood - log_likelihood_old) < tol:\n",
    "                break\n",
    "            log_likelihood_old = log_likelihood\n",
    "\n",
    "        # Store parameters for feature j\n",
    "        cGMM_params.append({\n",
    "            'means': means,\n",
    "            'variances': variances,\n",
    "            'weights': weights\n",
    "        })\n",
    "\n",
    "    return cGMM_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "221f9348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign clusters based on cGMM\n",
    "def assign_clusters_cgmm(X, cGMM_params, K):\n",
    "    num_samples, num_features = X.shape\n",
    "    cluster_posteriors = np.zeros((num_samples, K))\n",
    "\n",
    "    # Compute log-posteriors for each component\n",
    "    for j in range(num_features):\n",
    "        X_j = X[:, j]\n",
    "        params = cGMM_params[j]\n",
    "        means = params['means']\n",
    "        variances = params['variances']\n",
    "        weights = params['weights']\n",
    "\n",
    "        # Compute Gaussian densities\n",
    "        densities = np.zeros((num_samples, K))\n",
    "        for k in range(K):\n",
    "            coef = 1 / np.sqrt(2 * np.pi * variances[k])\n",
    "            exponent = -0.5 * ((X_j - means[k]) ** 2) / variances[k]\n",
    "            densities[:, k] = coef * np.exp(exponent)\n",
    "\n",
    "        # Compute weighted densities\n",
    "        weighted_densities = densities * weights\n",
    "        total_density = np.sum(weighted_densities, axis=1, keepdims=True)\n",
    "        responsibilities = weighted_densities / (total_density + 1e-300)\n",
    "\n",
    "        # Accumulate log-posteriors\n",
    "        cluster_posteriors += np.log(weighted_densities + 1e-300)\n",
    "\n",
    "    # Assign clusters based on maximum posterior probability\n",
    "    cluster_assignments = np.argmax(cluster_posteriors, axis=1)\n",
    "    return cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc20349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for evaluation\n",
    "def compute_confusion_matrix(y_true, y_pred, num_clusters, num_classes):\n",
    "    #Compute the confusion matrix for clustering.\n",
    "    confusion_matrix = np.zeros((num_clusters, num_classes), dtype=int)\n",
    "    for i in range(len(y_true)):\n",
    "        cluster = y_pred[i]\n",
    "        label = y_true[i]\n",
    "        confusion_matrix[cluster, label] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "def compute_cluster_consistency(confusion_matrix):\n",
    "    #Compute the consistency for each cluster.\n",
    "    cluster_sizes = np.sum(confusion_matrix, axis=1)\n",
    "    max_class_counts = np.max(confusion_matrix, axis=1)\n",
    "    # Handle division by zero for empty clusters\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        cluster_consistency = np.divide(max_class_counts, cluster_sizes, \n",
    "                                        out=np.zeros_like(max_class_counts, dtype=float), \n",
    "                                        where=cluster_sizes != 0)\n",
    "    return cluster_consistency\n",
    "\n",
    "def compute_overall_consistency(cluster_consistency, cluster_sizes):\n",
    "    #Compute the overall consistency.\n",
    "    total_samples = np.sum(cluster_sizes)\n",
    "    overall_consistency = np.sum(cluster_consistency * cluster_sizes) / total_samples\n",
    "    return overall_consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48a7798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cGMM to training data\n",
    "K = 10\n",
    "cGMM_params = em_cgmm(X_train_pca, K)\n",
    "\n",
    "# Assign clusters to training data\n",
    "cluster_assignments_train = assign_clusters_cgmm(X_train_pca, cGMM_params, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cfe4056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster Consistencies for Training Data:\n",
      "Cluster 0: Consistency = 0.3172, Size = 8746\n",
      "Cluster 1: Consistency = 0.2101, Size = 15851\n",
      "Cluster 2: Consistency = 0.2035, Size = 4467\n",
      "Cluster 3: Consistency = 0.3478, Size = 736\n",
      "Cluster 4: Consistency = 0.2671, Size = 2434\n",
      "Cluster 5: Consistency = 0.2740, Size = 13982\n",
      "Cluster 6: Consistency = 0.3826, Size = 7388\n",
      "Cluster 7: Consistency = 0.2527, Size = 1828\n",
      "Cluster 8: Consistency = 0.2934, Size = 4356\n",
      "Cluster 9: Consistency = 0.9906, Size = 212\n",
      "\n",
      "Overall Consistency for Training Data: 0.2754\n"
     ]
    }
   ],
   "source": [
    "# Evaluate clustering on training data using cluster consistency\n",
    "num_clusters = K \n",
    "num_classes = 10 \n",
    "confusion_mat_train = compute_confusion_matrix(y_train, cluster_assignments_train, num_clusters, num_classes)\n",
    "cluster_consistency_train = compute_cluster_consistency(confusion_mat_train)\n",
    "cluster_sizes_train = np.sum(confusion_mat_train, axis=1)\n",
    "overall_consistency_train = compute_overall_consistency(cluster_consistency_train, cluster_sizes_train)\n",
    "\n",
    "    \n",
    "print(\"\\nCluster Consistencies for Training Data:\")\n",
    "for i, consistency in enumerate(cluster_consistency_train):\n",
    "    print(f\"Cluster {i}: Consistency = {consistency:.4f}, Size = {cluster_sizes_train[i]}\")\n",
    "\n",
    "print(f\"\\nOverall Consistency for Training Data: {overall_consistency_train:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b2a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
