{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fb94fc",
   "metadata": {},
   "source": [
    "# MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a139bc-2740-4b0c-ace9-2a3bbdf2e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from mtcnn import MTCNN\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import logging\n",
    "\n",
    "#from evaluation_utils import evaluate_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d0a562-a3e6-45ae-af02-94a22c48c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='mtcnn_evaluation.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411f9d8d-5169-4f05-8edb-cbae0748d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2]) \n",
    "    y2 = min(box1[3], box2[3]) \n",
    "\n",
    "    intersection = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "\n",
    "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "\n",
    "    union = box1_area + box2_area - intersection\n",
    "\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def calc_precision(true_positives, false_positives):\n",
    "    if true_positives + false_positives > 0:\n",
    "        return true_positives / (true_positives + false_positives)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calc_recall(true_positives, false_negatives):\n",
    "    if true_positives + false_negatives > 0:\n",
    "        return true_positives / (true_positives + false_negatives)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calc_f1_score(precision, recall):\n",
    "    if precision + recall > 0:\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def evaluate_predictions(predictions, ground_truth, iou_threshold=0.5):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    matched = [False] * len(ground_truth)\n",
    "\n",
    "    for pred in predictions:\n",
    "        matched_any = False\n",
    "        for i, gt in enumerate(ground_truth):\n",
    "            if not matched[i] and calc_iou(pred, gt) >= iou_threshold:\n",
    "                matched[i] = True\n",
    "                matched_any = True\n",
    "                true_positives += 1\n",
    "                break\n",
    "        if not matched_any:\n",
    "            false_positives += 1\n",
    "    false_negatives = len(ground_truth) - sum(matched)\n",
    "\n",
    "    precision = calc_precision(true_positives, false_positives)\n",
    "    recall = calc_recall(true_positives, false_negatives)\n",
    "    f1_score = calc_f1_score(precision, recall)\n",
    "\n",
    "    return precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b996a73-a68d-4764-9999-9227da04dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mtcnn_with_predictions(json_path, output_metrics_path, predictions_output_path, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate MTCNN model on a subset of images from the given JSON annotation file,\n",
    "    and save the model's predictions in a separate JSON file.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing annotations.\n",
    "        output_metrics_path (str): Path to save the evaluation metrics as a JSON file.\n",
    "        predictions_output_path (str): Path to save the model predictions as a JSON file.\n",
    "        iou_threshold (float): IoU threshold to consider a prediction as true positive.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints mean precision, recall, and F1-score, saves metrics and predictions to JSON files.\n",
    "    \"\"\"\n",
    "    # Load the annotations\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        annotations = json.load(json_file)\n",
    "\n",
    "    # Initialize the MTCNN model\n",
    "    detector = MTCNN()\n",
    "    logger.info(\"Initialized MTCNN detector.\")\n",
    "\n",
    "    # Metrics for all images\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Store predictions\n",
    "    predictions_data = []\n",
    "\n",
    "    # Iterate over all image annotations with a progress bar\n",
    "    for index, image_annotation in enumerate(tqdm(annotations, desc=\"Evaluating Images\")):\n",
    "        # Extract ground truths and convert to [x1, y1, x2, y2] format\n",
    "        ground_truths = []\n",
    "        for gt in image_annotation.get('image_info', []):\n",
    "            bbox = gt.get('bbox', [])\n",
    "            if len(bbox) != 4:\n",
    "                logger.warning(f\"Invalid bbox format in image {image_annotation.get('image_path', 'unknown')}. Skipping this ground truth.\")\n",
    "                continue\n",
    "            x, y, w, h = bbox\n",
    "            x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)\n",
    "            # Ensure coordinates are positive\n",
    "            x1, y1, x2, y2 = max(0, x1), max(0, y1), max(0, x2), max(0, y2)\n",
    "            ground_truths.append([x1, y1, x2, y2])\n",
    "\n",
    "        # Load the image\n",
    "        image_path = image_annotation.get('image_path', '')\n",
    "        if not image_path:\n",
    "            logger.warning(f\"No image path provided for annotation index {index}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            logger.error(f\"Image file does not exist: {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            logger.error(f\"Error: Could not load image at {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Convert image from BGR (OpenCV format) to RGB (MTCNN expects RGB)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get predictions using MTCNN\n",
    "        detections = detector.detect_faces(image_rgb)\n",
    "        predictions = []\n",
    "        for face in detections:\n",
    "            # MTCNN returns 'box' as [x, y, width, height]\n",
    "            box = face.get('box', [])\n",
    "            if len(box) != 4:\n",
    "                logger.warning(f\"Invalid detection box format in image {image_path}. Skipping this detection.\")\n",
    "                continue\n",
    "            x, y, w, h = box\n",
    "            x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)\n",
    "            # Ensure coordinates are within image boundaries\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(image.shape[1] - 1, x2), min(image.shape[0] - 1, y2)\n",
    "            predictions.append([x1, y1, x2, y2])\n",
    "\n",
    "        # Save predictions for this image\n",
    "        predictions_data.append({\n",
    "            \"image_path\": image_path,\n",
    "            \"predicted_boxes\": predictions\n",
    "        })\n",
    "\n",
    "        # Calculate metrics\n",
    "        if not predictions and not ground_truths:\n",
    "            precision, recall, f1_score = 0, 0, 0\n",
    "        elif not predictions:\n",
    "            precision, recall, f1_score = 0, 0, 0\n",
    "        elif not ground_truths:\n",
    "            precision, recall, f1_score = 0, 0, 0\n",
    "        else:\n",
    "            precision, recall, f1_score = evaluate_predictions(predictions, ground_truths, iou_threshold)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "    # Calculate mean metrics\n",
    "    mean_precision = sum(precision_scores) / len(precision_scores) if precision_scores else 0\n",
    "    mean_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0\n",
    "    mean_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
    "\n",
    "    # Print the results\n",
    "    logger.info(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    logger.info(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    logger.info(f\"Mean F1-Score: {mean_f1:.4f}\")\n",
    "\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1-Score: {mean_f1:.4f}\")\n",
    "\n",
    "    # Save metrics to a JSON file\n",
    "    output_data = {\n",
    "        \"mean_precision\": mean_precision,\n",
    "        \"mean_recall\": mean_recall,\n",
    "        \"mean_f1_score\": mean_f1,\n",
    "        \"individual_scores\": {\n",
    "            \"precision_scores\": [float(p) for p in precision_scores],\n",
    "            \"recall_scores\": [float(r) for r in recall_scores],\n",
    "            \"f1_scores\": [float(f) for f in f1_scores]\n",
    "        },\n",
    "        \"predictions\": predictions_data\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_metrics_path, 'w') as output_file:\n",
    "            json.dump(output_data, output_file, indent=4)\n",
    "        logger.info(f\"Metrics saved to {output_metrics_path}\")\n",
    "        print(f\"Metrics saved to {output_metrics_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving metrics to {output_metrics_path}: {e}\")\n",
    "        print(f\"Error saving metrics to {output_metrics_path}: {e}\")\n",
    "\n",
    "    # Save predictions to a separate JSON file\n",
    "    try:\n",
    "        with open(predictions_output_path, 'w') as pred_file:\n",
    "            json.dump(predictions_data, pred_file, indent=4)\n",
    "        logger.info(f\"Predictions saved to {predictions_output_path}\")\n",
    "        print(f\"Predictions saved to {predictions_output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving predictions to {predictions_output_path}: {e}\")\n",
    "        print(f\"Error saving predictions to {predictions_output_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a58fa89f-6214-42f0-85ff-53f948f5f36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|███| 1122/1122 [10:18<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.7648\n",
      "Mean Recall: 0.8146\n",
      "Mean F1-Score: 0.7807\n",
      "Metrics saved to Outputs/metrics_easy_num_faces_mtcnn.json\n",
      "Predictions saved to Outputs/predictions_easy_num_faces_mtcnn.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_num_faces_easy.json\"\n",
    "output_path_metrics = \"Outputs/metrics_easy_num_faces_mtcnn.json\"\n",
    "output_path_predictions = \"Outputs/predictions_easy_num_faces_mtcnn.json\"\n",
    "\n",
    "evaluate_mtcnn_with_predictions(json_path, output_path_metrics, output_path_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc3ba6e7-c7bf-45d0-bb81-94afefaf8004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|███| 1089/1089 [09:42<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.8388\n",
      "Mean Recall: 0.7122\n",
      "Mean F1-Score: 0.7448\n",
      "Metrics saved to Outputs/metrics_medium_num_faces_mtcnn.json\n",
      "Predictions saved to Outputs/predictions_medium_num_faces_mtcnn.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_num_faces_medium.json\"\n",
    "output_path_metrics = \"Outputs/metrics_medium_num_faces_mtcnn.json\"\n",
    "output_path_predictions = \"Outputs/predictions_medium_num_faces_mtcnn.json\"\n",
    "\n",
    "evaluate_mtcnn_with_predictions(json_path, output_path_metrics, output_path_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b574f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|███| 1015/1015 [10:21<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.8954\n",
      "Mean Recall: 0.4972\n",
      "Mean F1-Score: 0.5915\n",
      "Metrics saved to Outputs/metrics_hard_num_faces_mtcnn.json\n",
      "Predictions saved to Outputs/predictions_hard_num_faces_mtcnn.json\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_num_faces_hard.json\"\n",
    "output_path_metrics = \"Outputs/metrics_hard_num_faces_mtcnn.json\"\n",
    "output_path_predictions = \"Outputs/predictions_hard_num_faces_mtcnn.json\"\n",
    "\n",
    "evaluate_mtcnn_with_predictions(json_path, output_path_metrics, output_path_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a7ba49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def evaluate_mtcnn_with_random_subset(\n",
    "    json_path,\n",
    "    output_metrics_path,\n",
    "    predictions_output_path,\n",
    "    iou_threshold=0.5,\n",
    "    num_samples=None,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate MTCNN model on a subset of images from the given JSON annotation file,\n",
    "    optionally sampling a random subset, and save the model's predictions in a separate JSON file.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing annotations.\n",
    "        output_metrics_path (str): Path to save the evaluation metrics as a JSON file.\n",
    "        predictions_output_path (str): Path to save the model predictions as a JSON file.\n",
    "        iou_threshold (float, optional): IoU threshold to consider a prediction as true positive. Defaults to 0.5.\n",
    "        num_samples (int, optional): Number of random samples to evaluate. If None, evaluate on all images. Defaults to None.\n",
    "        seed (int, optional): Random seed for reproducibility when sampling. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints mean precision, recall, and F1-score, saves metrics and predictions to JSON files.\n",
    "    \"\"\"\n",
    "    # Load the annotations\n",
    "    try:\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            annotations = json.load(json_file)\n",
    "        logger.info(f\"Loaded {len(annotations)} annotations from {json_path}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load annotations from {json_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Randomly sample the dataset if num_samples is specified\n",
    "    if num_samples is not None:\n",
    "        random.seed(seed)\n",
    "        sampled_size = min(num_samples, len(annotations))\n",
    "        annotations = random.sample(annotations, sampled_size)\n",
    "        logger.info(f\"Randomly sampled {sampled_size} annotations with seed {seed}.\")\n",
    "\n",
    "    # Initialize the MTCNN model\n",
    "    detector = MTCNN()\n",
    "    logger.info(\"Initialized MTCNN detector.\")\n",
    "\n",
    "    # Metrics for all images\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Store predictions\n",
    "    predictions_data = []\n",
    "\n",
    "    # Iterate over all image annotations with a progress bar\n",
    "    for index, image_annotation in enumerate(tqdm(annotations, desc=\"Evaluating Images\")):\n",
    "        # Extract ground truths and convert to [x1, y1, x2, y2] format\n",
    "        ground_truths = []\n",
    "        for gt in image_annotation.get('image_info', []):\n",
    "            bbox = gt.get('bbox', [])\n",
    "            if len(bbox) != 4:\n",
    "                logger.warning(f\"Invalid bbox format in image {image_annotation.get('image_path', 'unknown')}. Skipping this ground truth.\")\n",
    "                continue\n",
    "            x, y, w, h = bbox\n",
    "            x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)\n",
    "            # Ensure coordinates are positive\n",
    "            x1, y1, x2, y2 = max(0, x1), max(0, y1), max(0, x2), max(0, y2)\n",
    "            ground_truths.append([x1, y1, x2, y2])\n",
    "\n",
    "        # Load the image\n",
    "        image_path = image_annotation.get('image_path', '')\n",
    "        if not image_path:\n",
    "            logger.warning(f\"No image path provided for annotation index {index}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            logger.error(f\"Image file does not exist: {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            logger.error(f\"Error: Could not load image at {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Convert image from BGR (OpenCV format) to RGB (MTCNN expects RGB)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get predictions using MTCNN\n",
    "        detections = detector.detect_faces(image_rgb)\n",
    "        predictions = []\n",
    "        for face in detections:\n",
    "            # MTCNN returns 'box' as [x, y, width, height]\n",
    "            box = face.get('box', [])\n",
    "            if len(box) != 4:\n",
    "                logger.warning(f\"Invalid detection box format in image {image_path}. Skipping this detection.\")\n",
    "                continue\n",
    "            x, y, w, h = box\n",
    "            x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)\n",
    "            # Ensure coordinates are within image boundaries\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(image.shape[1] - 1, x2), min(image.shape[0] - 1, y2)\n",
    "            predictions.append([x1, y1, x2, y2])\n",
    "\n",
    "        # Save predictions for this image\n",
    "        predictions_data.append({\n",
    "            \"image_path\": image_path,\n",
    "            \"predicted_boxes\": predictions\n",
    "        })\n",
    "\n",
    "        # Calculate metrics\n",
    "        if not predictions and not ground_truths:\n",
    "            precision, recall, f1_score = 1.0, 1.0, 1.0  # Perfect score when both are empty\n",
    "        elif not predictions:\n",
    "            precision, recall, f1_score = 0.0, 0.0, 0.0\n",
    "        elif not ground_truths:\n",
    "            precision, recall, f1_score = 0.0, 0.0, 0.0\n",
    "        else:\n",
    "            precision, recall, f1_score = evaluate_predictions(predictions, ground_truths, iou_threshold)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "    # Calculate mean metrics\n",
    "    num_evaluated = len(precision_scores)\n",
    "    mean_precision = sum(precision_scores) / num_evaluated if num_evaluated else 0\n",
    "    mean_recall = sum(recall_scores) / num_evaluated if num_evaluated else 0\n",
    "    mean_f1 = sum(f1_scores) / num_evaluated if num_evaluated else 0\n",
    "\n",
    "    # Print the results\n",
    "    logger.info(f\"Evaluated {num_evaluated} images.\")\n",
    "    logger.info(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    logger.info(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    logger.info(f\"Mean F1-Score: {mean_f1:.4f}\")\n",
    "\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1-Score: {mean_f1:.4f}\")\n",
    "\n",
    "    # Save metrics to a JSON file\n",
    "    output_data = {\n",
    "        \"mean_precision\": mean_precision,\n",
    "        \"mean_recall\": mean_recall,\n",
    "        \"mean_f1_score\": mean_f1,\n",
    "        \"individual_scores\": {\n",
    "            \"precision_scores\": [float(p) for p in precision_scores],\n",
    "            \"recall_scores\": [float(r) for r in recall_scores],\n",
    "            \"f1_scores\": [float(f) for f in f1_scores]\n",
    "        },\n",
    "        \"predictions\": predictions_data\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_metrics_path, 'w') as output_file:\n",
    "            json.dump(output_data, output_file, indent=4)\n",
    "        logger.info(f\"Metrics saved to {output_metrics_path}\")\n",
    "        print(f\"Metrics saved to {output_metrics_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving metrics to {output_metrics_path}: {e}\")\n",
    "        print(f\"Error saving metrics to {output_metrics_path}: {e}\")\n",
    "\n",
    "    # Save predictions to a separate JSON file\n",
    "    try:\n",
    "        with open(predictions_output_path, 'w') as pred_file:\n",
    "            json.dump(predictions_data, pred_file, indent=4)\n",
    "        logger.info(f\"Predictions saved to {predictions_output_path}\")\n",
    "        print(f\"Predictions saved to {predictions_output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving predictions to {predictions_output_path}: {e}\")\n",
    "        print(f\"Error saving predictions to {predictions_output_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd324b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████| 700/700 [17:32<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.8646\n",
      "Mean Recall: 0.8900\n",
      "Mean F1-Score: 0.8659\n",
      "Metrics saved to Outputs/metrics_easy_blur_mtcnn.json\n",
      "Predictions saved to Outputs/predictions_easy_blur_mtcnn.json\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_easy_blur.json\"\n",
    "output_path_metrics = \"Outputs/metrics_easy_blur_mtcnn.json\"\n",
    "output_path_predictions = \"Outputs/predictions_easy_blur_mtcnn.json\"\n",
    "\n",
    "evaluate_mtcnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c204f8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████| 700/700 [17:42<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.8229\n",
      "Mean Recall: 0.7554\n",
      "Mean F1-Score: 0.7718\n",
      "Metrics saved to Outputs/metrics_medium_blur_mtcnn.json\n",
      "Predictions saved to Outputs/predictions_medium_blur_mtcnn.json\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_medium_blur.json\"\n",
    "output_path_metrics = \"Outputs/metrics_medium_blur_mtcnn.json\"\n",
    "output_path_predictions = \"Outputs/predictions_medium_blur_mtcnn.json\"\n",
    "\n",
    "evaluate_mtcnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea2c2e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████| 700/700 [19:37<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.8111\n",
      "Mean Recall: 0.4078\n",
      "Mean F1-Score: 0.5043\n",
      "Metrics saved to Outputs/metrics_hard_blur_mtcnn.json\n",
      "Predictions saved to Outputs/predictions_hard_blur_mtcnn.json\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_hard_blur.json\"\n",
    "output_path_metrics = \"Outputs/metrics_hard_blur_mtcnn.json\"\n",
    "output_path_predictions = \"Outputs/predictions_hard_blur_mtcnn.json\"\n",
    "\n",
    "evaluate_mtcnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cd31268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████| 300/300 [02:48<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.8750\n",
      "Mean Recall: 0.8600\n",
      "Mean F1-Score: 0.8536\n",
      "Metrics saved to Outputs/metrics_easy_occlusion_mtcnn.json\n",
      "Predictions saved to Outputs/predictions_easy_occlusion_mtcnn.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_easy_occlusion.json\"\n",
    "output_path_metrics = \"Outputs/metrics_easy_occlusion_mtcnn.json\"\n",
    "output_path_predictions = \"Outputs/predictions_easy_occlusion_mtcnn.json\"\n",
    "\n",
    "evaluate_mtcnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ae77688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████| 300/300 [02:29<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.7574\n",
      "Mean Recall: 0.6025\n",
      "Mean F1-Score: 0.6401\n",
      "Metrics saved to Outputs/metrics_medium_occlusion_mtcnn.json\n",
      "Predictions saved to Outputs/predictions_medium_occlusion_mtcnn.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_medium_occlusion.json\"\n",
    "output_path_metrics = \"Outputs/metrics_medium_occlusion_mtcnn.json\"\n",
    "output_path_predictions = \"Outputs/predictions_medium_occlusion_mtcnn.json\"\n",
    "\n",
    "evaluate_mtcnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9379a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████| 300/300 [02:51<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.8145\n",
      "Mean Recall: 0.4126\n",
      "Mean F1-Score: 0.5098\n",
      "Metrics saved to Outputs/metrics_hard_occlusion_mtcnn.json\n",
      "Predictions saved to Outputs/predictions_hard_occlusion_mtcnn.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_hard_occlusion.json\"\n",
    "output_path_metrics = \"Outputs/metrics_hard_occlusion_mtcnn.json\"\n",
    "output_path_predictions = \"Outputs/predictions_hard_occlusion_mtcnn.json\"\n",
    "\n",
    "evaluate_mtcnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa2321fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████| 400/400 [03:21<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.8445\n",
      "Mean Recall: 0.7231\n",
      "Mean F1-Score: 0.7462\n",
      "Metrics saved to Outputs/metrics_normal_illumination_mtcnn.json\n",
      "Predictions saved to Outputs/predictions_normal_illumination_mtcnn.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_normal_illumination.json\"\n",
    "output_path_metrics = \"Outputs/metrics_normal_illumination_mtcnn.json\"\n",
    "output_path_predictions = \"Outputs/predictions_normal_illumination_mtcnn.json\"\n",
    "\n",
    "evaluate_mtcnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d9073e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████| 400/400 [03:46<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.8374\n",
      "Mean Recall: 0.5526\n",
      "Mean F1-Score: 0.6140\n",
      "Metrics saved to Outputs/metrics_extreme_illumination_mtcnn.json\n",
      "Predictions saved to Outputs/predictions_extreme_illumination_mtcnn.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_extreme_illumination.json\"\n",
    "output_path_metrics = \"Outputs/metrics_extreme_illumination_mtcnn.json\"\n",
    "output_path_predictions = \"Outputs/predictions_extreme_illumination_mtcnn.json\"\n",
    "\n",
    "evaluate_mtcnn_with_random_subset(json_path, output_path_metrics, output_path_predictions,num_samples=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2581ad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring Prediction Times: 100%|█| 100/100 [00:52<00:00,  1.91"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average prediction time: 0.5005 seconds per image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_prediction_time_mtcnn(json_path, max_images=100, seed=42):\n",
    "    \"\"\"\n",
    "    Calculate the average prediction time of the MTCNN model on a random subset of images.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing annotations.\n",
    "        max_images (int): Number of images to process.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        float: The average prediction time per image in seconds.\n",
    "    \"\"\"\n",
    "    # Check if the JSON file exists\n",
    "    if not os.path.exists(json_path):\n",
    "        logger.error(f\"JSON file does not exist: {json_path}\")\n",
    "        print(f\"Error: JSON file does not exist: {json_path}\")\n",
    "        return None\n",
    "\n",
    "    # Load the annotations\n",
    "    try:\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            annotations = json.load(json_file)\n",
    "            logger.info(f\"Loaded {len(annotations)} annotations from {json_path}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error decoding JSON file: {e}\")\n",
    "        print(f\"Error decoding JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Randomly select a subset of the data\n",
    "    random.seed(seed)\n",
    "    if len(annotations) > max_images:\n",
    "        annotations_subset = random.sample(annotations, max_images)\n",
    "        logger.info(f\"Selected {max_images} random images for evaluation.\")\n",
    "    else:\n",
    "        annotations_subset = annotations\n",
    "        logger.warning(f\"Dataset contains fewer than {max_images} images. Using all available images ({len(annotations_subset)}).\")\n",
    "\n",
    "    # Initialize the MTCNN model\n",
    "    detector = MTCNN()\n",
    "    logger.info(\"Initialized MTCNN detector.\")\n",
    "\n",
    "    # Measure prediction times\n",
    "    prediction_times = []\n",
    "\n",
    "    for image_annotation in tqdm(annotations_subset, desc=\"Measuring Prediction Times\"):\n",
    "        # Load the image\n",
    "        image_path = image_annotation.get('image_path', '')\n",
    "        if not image_path:\n",
    "            logger.warning(\"No image path provided for an annotation. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            logger.error(f\"Image file does not exist: {image_path}. Skipping.\")\n",
    "            print(f\"Error: Image file does not exist: {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            logger.error(f\"Error: Could not load image at {image_path}. Skipping.\")\n",
    "            print(f\"Error: Could not load image at {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Convert image from BGR (OpenCV format) to RGB (MTCNN expects RGB)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Make predictions using MTCNN\n",
    "        detections = detector.detect_faces(image_rgb)\n",
    "\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate prediction time\n",
    "        prediction_time = end_time - start_time\n",
    "        prediction_times.append(prediction_time)\n",
    "\n",
    "    # Calculate the average prediction time\n",
    "    if prediction_times:\n",
    "        average_time = sum(prediction_times) / len(prediction_times)\n",
    "        logger.info(f\"Average prediction time: {average_time:.4f} seconds per image\")\n",
    "        print(f\"Average prediction time: {average_time:.4f} seconds per image\")\n",
    "        return average_time\n",
    "    else:\n",
    "        logger.warning(\"No valid images were processed.\")\n",
    "        print(\"No valid images were processed.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    json_path = \"Data/WIDER FACE Validation Set/wider_face_val_annotations.json\"\n",
    "    calculate_average_prediction_time_mtcnn(json_path, max_images=100, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5695e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mtcnn_env)",
   "language": "python",
   "name": "mtcnn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
