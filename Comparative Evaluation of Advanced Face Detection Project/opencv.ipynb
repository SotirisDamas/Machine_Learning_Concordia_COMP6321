{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676b0940-bbdc-4c08-9e93-5dd9d0ef1187",
   "metadata": {},
   "source": [
    "# OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67e3d71a-9edd-4fc3-b142-352bba049d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb0280e8-672e-41bb-b870-aabae669fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='opencv_evaluation.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fffcd99-c69d-4e4a-a7f8-7ed22579fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(box1, box2):\n",
    "    # Calculate Intersection over Union\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])  # min for intersection\n",
    "    y2 = min(box1[3], box2[3])  # min for intersection\n",
    "\n",
    "    intersection = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "\n",
    "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "\n",
    "    union = box1_area + box2_area - intersection\n",
    "\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def calc_precision(true_positives, false_positives):\n",
    "    return true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "\n",
    "def calc_recall(true_positives, false_negatives):\n",
    "    return true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "def calc_f1_score(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "def evaluate_predictions(predictions, ground_truth, iou_threshold=0.5):\n",
    "    # Evaluate predictions against ground truth\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    matched = [False] * len(ground_truth)\n",
    "\n",
    "    for pred in predictions:\n",
    "        matched_any = False\n",
    "        for i, gt in enumerate(ground_truth):\n",
    "            if not matched[i] and calc_iou(pred, gt) >= iou_threshold:\n",
    "                matched[i] = True\n",
    "                matched_any = True\n",
    "                true_positives += 1\n",
    "                break\n",
    "        if not matched_any:\n",
    "            false_positives += 1\n",
    "    false_negatives = len(ground_truth) - sum(matched)\n",
    "\n",
    "    precision = calc_precision(true_positives, false_positives)\n",
    "    recall = calc_recall(true_positives, false_negatives)\n",
    "    f1_score = calc_f1_score(precision, recall)\n",
    "\n",
    "    return precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70dc3a2f-2530-4dc0-b915-8ce196d96de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_opencv_dnn_with_predictions(json_path, output_metrics_path, predictions_output_path, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate OpenCV DNN face detector on images from the given JSON annotation file,\n",
    "    and save the model's predictions in a separate JSON file.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing annotations.\n",
    "        output_metrics_path (str): Path to save the evaluation metrics as a JSON file.\n",
    "        predictions_output_path (str): Path to save the model predictions as a JSON file.\n",
    "        iou_threshold (float): IoU threshold to consider a prediction as true positive.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the annotations\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        annotations = json.load(json_file)\n",
    "\n",
    "    # Initialize the OpenCV DNN face detector\n",
    "    model_path = r\"C:\\Users\\sotir\\Project\\models\\OpenCV\\deploy.prototxt\"\n",
    "    weights_path = r\"C:\\Users\\sotir\\Project\\models\\OpenCV\\res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "    net = cv2.dnn.readNetFromCaffe(model_path, weights_path)\n",
    "    logger.info(\"Initialized OpenCV DNN face detector.\")\n",
    "\n",
    "    # Metrics for all images\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Store predictions\n",
    "    predictions_data = []\n",
    "\n",
    "    # Iterate over all image annotations with a progress bar\n",
    "    for index, image_annotation in enumerate(tqdm(annotations, desc=\"Evaluating Images\")):\n",
    "        # Extract ground truths\n",
    "        ground_truths = []\n",
    "        for gt in image_annotation.get('image_info', []):\n",
    "            bbox = gt.get('bbox', [])\n",
    "            if len(bbox) != 4:\n",
    "                logger.warning(f\"Invalid bbox format in image {image_annotation.get('image_path', 'unknown')}. Skipping this ground truth.\")\n",
    "                continue\n",
    "            x, y, w, h = bbox\n",
    "            x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)\n",
    "            ground_truths.append([x1, y1, x2, y2])\n",
    "\n",
    "        # Load the image\n",
    "        image_path = image_annotation.get('image_path', '')\n",
    "        if not image_path:\n",
    "            logger.warning(f\"No image path provided for annotation index {index}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            logger.error(f\"Image file does not exist: {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            logger.error(f\"Error: Could not load image at {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        (h, w) = image.shape[:2]\n",
    "\n",
    "        # Preprocess the image for the DNN model\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0,\n",
    "                                     (300, 300), (104.0, 177.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "\n",
    "        predictions = []\n",
    "        # Iterate over detections\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > 0.5:\n",
    "                # Get bounding box coordinates\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                x1, y1, x2, y2 = box.astype(\"int\")\n",
    "                # Ensure coordinates are within image boundaries\n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(w - 1, x2), min(h - 1, y2)\n",
    "                predictions.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "\n",
    "        # Save predictions for this image\n",
    "        predictions_data.append({\n",
    "            \"image_path\": image_path,\n",
    "            \"predicted_boxes\": predictions\n",
    "        })\n",
    "\n",
    "        # Calculate metrics\n",
    "        if not predictions and not ground_truths:\n",
    "            precision, recall, f1_score = 0, 0, 0\n",
    "        elif not predictions:\n",
    "            precision, recall, f1_score = 0, 0, 0\n",
    "        elif not ground_truths:\n",
    "            precision, recall, f1_score = 0, 0, 0\n",
    "        else:\n",
    "            precision, recall, f1_score = evaluate_predictions(predictions, ground_truths, iou_threshold)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "    # Calculate mean metrics\n",
    "    mean_precision = sum(precision_scores) / len(precision_scores) if precision_scores else 0\n",
    "    mean_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0\n",
    "    mean_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
    "\n",
    "    # Log and print the results\n",
    "    logger.info(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    logger.info(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    logger.info(f\"Mean F1-Score: {mean_f1:.4f}\")\n",
    "\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1-Score: {mean_f1:.4f}\")\n",
    "\n",
    "    # Save metrics to a JSON file\n",
    "    output_data = {\n",
    "        \"mean_precision\": mean_precision,\n",
    "        \"mean_recall\": mean_recall,\n",
    "        \"mean_f1_score\": mean_f1,\n",
    "        \"individual_scores\": {\n",
    "            \"precision_scores\": precision_scores,\n",
    "            \"recall_scores\": recall_scores,\n",
    "            \"f1_scores\": f1_scores\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_metrics_path, 'w') as output_file:\n",
    "            json.dump(output_data, output_file, indent=4)\n",
    "        logger.info(f\"Metrics saved to {output_metrics_path}\")\n",
    "        print(f\"Metrics saved to {output_metrics_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving metrics to {output_metrics_path}: {e}\")\n",
    "        print(f\"Error saving metrics to {output_metrics_path}: {e}\")\n",
    "\n",
    "    # Save predictions to a separate JSON file\n",
    "    try:\n",
    "        with open(predictions_output_path, 'w') as pred_file:\n",
    "            json.dump(predictions_data, pred_file, indent=4)\n",
    "        logger.info(f\"Predictions saved to {predictions_output_path}\")\n",
    "        print(f\"Predictions saved to {predictions_output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving predictions to {predictions_output_path}: {e}\")\n",
    "        print(f\"Error saving predictions to {predictions_output_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c540610-6f57-4d59-8d5a-595c0349874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|███████████████████████████████████████████████████████████| 1122/1122 [00:46<00:00, 24.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.7019\n",
      "Mean Recall: 0.7086\n",
      "Mean F1-Score: 0.7041\n",
      "Metrics saved to Outputs/OpenCV/metrics_easy_num_faces_opencv.json\n",
      "Predictions saved to Outputs/OpenCV/predictions_easy_num_faces_opencv.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_num_faces_easy.json\"\n",
    "output_path_metrics = \"Outputs/OpenCV/metrics_easy_num_faces_opencv.json\"\n",
    "output_path_predictions = \"Outputs/OpenCV/predictions_easy_num_faces_opencv.json\"\n",
    "\n",
    "evaluate_opencv_dnn_with_predictions(json_path, output_path_metrics, output_path_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e49e115e-6437-4417-afbf-fe27f30de69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|███████████████████████████████████████████████████████████| 1089/1089 [00:46<00:00, 23.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.6232\n",
      "Mean Recall: 0.4518\n",
      "Mean F1-Score: 0.5002\n",
      "Metrics saved to Outputs/OpenCV/metrics_medium_num_faces_opencv.json\n",
      "Predictions saved to Outputs/OpenCV/predictions_medium_num_faces_opencv.json\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_num_faces_medium.json\"\n",
    "output_path_metrics = \"Outputs/OpenCV/metrics_medium_num_faces_opencv.json\"\n",
    "output_path_predictions = \"Outputs/OpenCV/predictions_medium_num_faces_opencv.json\"\n",
    "\n",
    "evaluate_opencv_dnn_with_predictions(json_path, output_path_metrics, output_path_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ee59402-a720-42c6-ad25-dec47363c758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|███████████████████████████████████████████████████████████| 1015/1015 [00:43<00:00, 23.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.3421\n",
      "Mean Recall: 0.0814\n",
      "Mean F1-Score: 0.1154\n",
      "Metrics saved to Outputs/OpenCV/metrics_hard_num_faces_opencv.json\n",
      "Predictions saved to Outputs/OpenCV/predictions_hard_num_faces_opencv.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_num_faces_hard.json\"\n",
    "output_path_metrics = \"Outputs/OpenCV/metrics_hard_num_faces_opencv.json\"\n",
    "output_path_predictions = \"Outputs/OpenCV/predictions_hard_num_faces_opencv.json\"\n",
    "\n",
    "evaluate_opencv_dnn_with_predictions(json_path, output_path_metrics, output_path_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d5ff270-4fef-426d-98f4-48d6bba330c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def evaluate_opencv_dnn_with_random_subset(json_path, output_metrics_path, predictions_output_path, iou_threshold=0.5,num_samples=None, seed=42):\n",
    "    \"\"\"\n",
    "    Evaluate OpenCV DNN face detector on images from the given JSON annotation file,\n",
    "    and save the model's predictions in a separate JSON file.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing annotations.\n",
    "        output_metrics_path (str): Path to save the evaluation metrics as a JSON file.\n",
    "        predictions_output_path (str): Path to save the model predictions as a JSON file.\n",
    "        iou_threshold (float): IoU threshold to consider a prediction as true positive.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the annotations\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        annotations = json.load(json_file)\n",
    "\n",
    "    # Randomly sample the dataset if num_samples is specified\n",
    "    if num_samples is not None:\n",
    "        random.seed(seed)\n",
    "        sampled_size = min(num_samples, len(annotations))\n",
    "        annotations = random.sample(annotations, sampled_size)\n",
    "        logger.info(f\"Randomly sampled {sampled_size} annotations with seed {seed}.\")\n",
    "\n",
    "    # Initialize the OpenCV DNN face detector\n",
    "    model_path = r\"C:\\Users\\sotir\\Project\\models\\OpenCV\\deploy.prototxt\"\n",
    "    weights_path = r\"C:\\Users\\sotir\\Project\\models\\OpenCV\\res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "    net = cv2.dnn.readNetFromCaffe(model_path, weights_path)\n",
    "    logger.info(\"Initialized OpenCV DNN face detector.\")\n",
    "\n",
    "    # Metrics for all images\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Store predictions\n",
    "    predictions_data = []\n",
    "\n",
    "    # Iterate over all image annotations with a progress bar\n",
    "    for index, image_annotation in enumerate(tqdm(annotations, desc=\"Evaluating Images\")):\n",
    "        # Extract ground truths\n",
    "        ground_truths = []\n",
    "        for gt in image_annotation.get('image_info', []):\n",
    "            bbox = gt.get('bbox', [])\n",
    "            if len(bbox) != 4:\n",
    "                logger.warning(f\"Invalid bbox format in image {image_annotation.get('image_path', 'unknown')}. Skipping this ground truth.\")\n",
    "                continue\n",
    "            x, y, w, h = bbox\n",
    "            x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)\n",
    "            ground_truths.append([x1, y1, x2, y2])\n",
    "\n",
    "        # Load the image\n",
    "        image_path = image_annotation.get('image_path', '')\n",
    "        if not image_path:\n",
    "            logger.warning(f\"No image path provided for annotation index {index}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            logger.error(f\"Image file does not exist: {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            logger.error(f\"Error: Could not load image at {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        (h, w) = image.shape[:2]\n",
    "\n",
    "        # Preprocess the image for the DNN model\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0,\n",
    "                                     (300, 300), (104.0, 177.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "\n",
    "        predictions = []\n",
    "        # Iterate over detections\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > 0.5:\n",
    "                # Get bounding box coordinates\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                x1, y1, x2, y2 = box.astype(\"int\")\n",
    "                # Ensure coordinates are within image boundaries\n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(w - 1, x2), min(h - 1, y2)\n",
    "                predictions.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "\n",
    "        # Save predictions for this image\n",
    "        predictions_data.append({\n",
    "            \"image_path\": image_path,\n",
    "            \"predicted_boxes\": predictions\n",
    "        })\n",
    "\n",
    "        # Calculate metrics\n",
    "        if not predictions and not ground_truths:\n",
    "            precision, recall, f1_score = 0, 0, 0\n",
    "        elif not predictions:\n",
    "            precision, recall, f1_score = 0, 0, 0\n",
    "        elif not ground_truths:\n",
    "            precision, recall, f1_score = 0, 0, 0\n",
    "        else:\n",
    "            precision, recall, f1_score = evaluate_predictions(predictions, ground_truths, iou_threshold)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "    # Calculate mean metrics\n",
    "    mean_precision = sum(precision_scores) / len(precision_scores) if precision_scores else 0\n",
    "    mean_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0\n",
    "    mean_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
    "\n",
    "    # Log and print the results\n",
    "    logger.info(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    logger.info(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    logger.info(f\"Mean F1-Score: {mean_f1:.4f}\")\n",
    "\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1-Score: {mean_f1:.4f}\")\n",
    "\n",
    "    # Save metrics to a JSON file\n",
    "    output_data = {\n",
    "        \"mean_precision\": mean_precision,\n",
    "        \"mean_recall\": mean_recall,\n",
    "        \"mean_f1_score\": mean_f1,\n",
    "        \"individual_scores\": {\n",
    "            \"precision_scores\": precision_scores,\n",
    "            \"recall_scores\": recall_scores,\n",
    "            \"f1_scores\": f1_scores\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_metrics_path, 'w') as output_file:\n",
    "            json.dump(output_data, output_file, indent=4)\n",
    "        logger.info(f\"Metrics saved to {output_metrics_path}\")\n",
    "        print(f\"Metrics saved to {output_metrics_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving metrics to {output_metrics_path}: {e}\")\n",
    "        print(f\"Error saving metrics to {output_metrics_path}: {e}\")\n",
    "\n",
    "    # Save predictions to a separate JSON file\n",
    "    try:\n",
    "        with open(predictions_output_path, 'w') as pred_file:\n",
    "            json.dump(predictions_data, pred_file, indent=4)\n",
    "        logger.info(f\"Predictions saved to {predictions_output_path}\")\n",
    "        print(f\"Predictions saved to {predictions_output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving predictions to {predictions_output_path}: {e}\")\n",
    "        print(f\"Error saving predictions to {predictions_output_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33480c89-f57e-4c36-92e6-e5201f599a17",
   "metadata": {},
   "source": [
    "# Testing on Blur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f3b859-2429-48a8-882f-965104e01eb2",
   "metadata": {},
   "source": [
    "### Easy blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ea1e27f-2ed9-4081-96c6-4ed004ba83dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████████████████████████████████████████████████████████████| 700/700 [00:28<00:00, 24.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.8439\n",
      "Mean Recall: 0.7988\n",
      "Mean F1-Score: 0.8099\n",
      "Metrics saved to Outputs/OpenCV/metrics_easy_blur_opencv.json\n",
      "Predictions saved to Outputs/OpenCV/predictions_easy_blur_opencv.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_easy_blur.json\"\n",
    "output_path_metrics = \"Outputs/OpenCV/metrics_easy_blur_opencv.json\"\n",
    "output_path_predictions = \"Outputs/OpenCV/predictions_easy_blur_opencv.json\"\n",
    "\n",
    "evaluate_opencv_dnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d0bce-2765-48ea-b5e3-0a1466e54e1a",
   "metadata": {},
   "source": [
    "### Medium blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb13ea37-5056-42ca-a59e-4c4f962deab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████████████████████████████████████████████████████████████| 700/700 [00:27<00:00, 25.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.4996\n",
      "Mean Recall: 0.3252\n",
      "Mean F1-Score: 0.3699\n",
      "Metrics saved to Outputs/OpenCV/metrics_medium_blur_opencv.json\n",
      "Predictions saved to Outputs/OpenCV/predictions_medium_blur_opencv.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_medium_blur.json\"\n",
    "output_path_metrics = \"Outputs/OpenCV/metrics_medium_blur_opencv.json\"\n",
    "output_path_predictions = \"Outputs/OpenCV/predictions_medium_blur_opencv.json\"\n",
    "\n",
    "evaluate_opencv_dnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ab72f-de22-406e-96b7-22a7dbfa62bc",
   "metadata": {},
   "source": [
    "### Hard Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ec8d752-4ca8-4708-9029-8b61f60b448e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████████████████████████████████████████████████████████████| 700/700 [00:27<00:00, 25.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.2779\n",
      "Mean Recall: 0.0642\n",
      "Mean F1-Score: 0.0933\n",
      "Metrics saved to Outputs/OpenCV/metrics_hard_blur_opencv.json\n",
      "Predictions saved to Outputs/OpenCV/predictions_hard_blur_opencv.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_hard_blur.json\"\n",
    "output_path_metrics = \"Outputs/OpenCV/metrics_hard_blur_opencv.json\"\n",
    "output_path_predictions = \"Outputs/OpenCV/predictions_hard_blur_opencv.json\"\n",
    "\n",
    "evaluate_opencv_dnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056588f-158d-480b-b99a-fd2ab5981220",
   "metadata": {},
   "source": [
    "# Testing on occlusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8886c7d2-df38-401b-89cd-9c8d702ed304",
   "metadata": {},
   "source": [
    "### Easy occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "895bc917-aa6b-4c12-8f32-da959956503b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████████████████████████████████████████████████████████████| 300/300 [00:12<00:00, 24.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.6883\n",
      "Mean Recall: 0.6248\n",
      "Mean F1-Score: 0.6426\n",
      "Metrics saved to Outputs/OpenCV/metrics_easy_occlusion_opencv.json\n",
      "Predictions saved to Outputs/OpenCV/predictions_easy_occlusion_opencv.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_easy_occlusion.json\"\n",
    "output_path_metrics = \"Outputs/OpenCV/metrics_easy_occlusion_opencv.json\"\n",
    "output_path_predictions = \"Outputs/OpenCV/predictions_easy_occlusion_opencv.json\"\n",
    "\n",
    "evaluate_opencv_dnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db93e028-e02b-49f0-968c-5117fb0fc52d",
   "metadata": {},
   "source": [
    "### Medium occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54fbc017-503d-423f-b642-70e1633209de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████████████████████████████████████████████████████████████| 300/300 [00:12<00:00, 24.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.4353\n",
      "Mean Recall: 0.2473\n",
      "Mean F1-Score: 0.2870\n",
      "Metrics saved to Outputs/OpenCV/metrics_medium_occlusion_opencv.json\n",
      "Predictions saved to Outputs/OpenCV/predictions_medium_occlusion_opencv.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_medium_occlusion.json\"\n",
    "output_path_metrics = \"Outputs/OpenCV/metrics_medium_occlusion_opencv.json\"\n",
    "output_path_predictions = \"Outputs/OpenCV/predictions_medium_occlusion_opencv.json\"\n",
    "\n",
    "evaluate_opencv_dnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb0dc1-a346-41e7-8bc6-9fbeb01f5b54",
   "metadata": {},
   "source": [
    "### Hard occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "504d2369-8e46-4f8e-baf6-68c846a079a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████████████████████████████████████████████████████████████| 300/300 [00:12<00:00, 24.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.3649\n",
      "Mean Recall: 0.1058\n",
      "Mean F1-Score: 0.1434\n",
      "Metrics saved to Outputs/OpenCV/metrics_hard_occlusion_opencv.json\n",
      "Predictions saved to Outputs/OpenCV/predictions_hard_occlusion_opencv.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_hard_occlusion.json\"\n",
    "output_path_metrics = \"Outputs/OpenCV/metrics_hard_occlusion_opencv.json\"\n",
    "output_path_predictions = \"Outputs/OpenCV/predictions_hard_occlusion_opencv.json\"\n",
    "\n",
    "evaluate_opencv_dnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0abb219-eb98-4ae6-a047-c21446ccf398",
   "metadata": {},
   "source": [
    "# Testing on illumination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a3d55-e983-4ae7-a481-55bc642e7f0c",
   "metadata": {},
   "source": [
    "### Normal illumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dff9aaa8-e33b-494a-a8ca-896d24bef22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████████████████████████████████████████████████████████████| 400/400 [00:16<00:00, 24.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.6325\n",
      "Mean Recall: 0.5043\n",
      "Mean F1-Score: 0.5268\n",
      "Metrics saved to Outputs/OpenCV/metrics_normal_illumination_opencv.json\n",
      "Predictions saved to Outputs/OpenCV/predictions_normal_illumination_opencv.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_normal_illumination.json\"\n",
    "output_path_metrics = \"Outputs/OpenCV/metrics_normal_illumination_opencv.json\"\n",
    "output_path_predictions = \"Outputs/OpenCV/predictions_normal_illumination_opencv.json\"\n",
    "\n",
    "evaluate_opencv_dnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1dc506-b2cb-4fca-8dea-16da418f6ba0",
   "metadata": {},
   "source": [
    "### Extreme illumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d4ec287-25e6-4668-9cd7-6c57a11a6198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|█████████████████████████████████████████████████████████████| 400/400 [00:16<00:00, 24.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision: 0.4208\n",
      "Mean Recall: 0.2355\n",
      "Mean F1-Score: 0.2634\n",
      "Metrics saved to Outputs/OpenCV/metrics_extreme_illumination_opencv.json\n",
      "Predictions saved to Outputs/OpenCV/predictions_extreme_illumination_opencv.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"Data/WIDER FACE Validation Set/wider_face_extreme_illumination.json\"\n",
    "output_path_metrics = \"Outputs/OpenCV/metrics_extreme_illumination_opencv.json\"\n",
    "output_path_predictions = \"Outputs/OpenCV/predictions_extreme_illumination_opencv.json\"\n",
    "\n",
    "evaluate_opencv_dnn_with_random_subset(json_path, output_path_metrics, output_path_predictions, num_samples=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ae93145-948a-4390-afa8-afde7ff2ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(image_path, predictions, ground_truths=None):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not load image {image_path}\")\n",
    "        return\n",
    "    # Draw predictions in red\n",
    "    for pred in predictions:\n",
    "        x1, y1, x2, y2 = pred\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "    # Draw ground truths in green\n",
    "    if ground_truths:\n",
    "        for gt in ground_truths:\n",
    "            x1, y1, x2, y2 = gt\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    # Show image\n",
    "    cv2.imshow(\"Detections\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21e959f2-1d6c-402e-9762-58fa9e5531bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predictions and annotations\n",
    "with open(output_path_predictions, 'r') as pred_file:\n",
    "    predictions_data = json.load(pred_file)\n",
    "\n",
    "with open(json_path, 'r') as json_file:\n",
    "    annotations = json.load(json_file)\n",
    "\n",
    "# Choose an image to visualize\n",
    "sample_index = 150  # Change as needed\n",
    "image_path = predictions_data[sample_index][\"image_path\"]\n",
    "predictions = predictions_data[sample_index][\"predicted_boxes\"]\n",
    "\n",
    "# Find ground truths for the image\n",
    "ground_truths = []\n",
    "for ann in annotations:\n",
    "    if ann[\"image_path\"] == image_path:\n",
    "        for gt in ann.get(\"image_info\", []):\n",
    "            bbox = gt.get('bbox', [])\n",
    "            x, y, w, h = bbox\n",
    "            x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)\n",
    "            ground_truths.append([x1, y1, x2, y2])\n",
    "        break\n",
    "\n",
    "visualize_detections(image_path, predictions, ground_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "248cb880-dccf-46c8-9055-4e0868122e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring Prediction Times: 100%|████████████████████████████████████████████████████| 100/100 [00:03<00:00, 26.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average prediction time (OpenCV DNN): 0.0282 seconds per image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def calculate_average_prediction_time_opencv_dnn(json_path, prototxt_path, model_path, confidence_threshold=0.5, max_images=100, seed=42):\n",
    "    \"\"\"\n",
    "    Calculate the average prediction time of OpenCV's DNN-based face detector on a random subset of images.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing annotations.\n",
    "        prototxt_path (str): Path to the DNN model's prototxt file.\n",
    "        model_path (str): Path to the DNN model's weights file.\n",
    "        confidence_threshold (float): Minimum confidence threshold to filter weak detections.\n",
    "        max_images (int): Number of images to process.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        float: The average prediction time per image in seconds.\n",
    "    \"\"\"\n",
    "    # Check if the JSON file exists\n",
    "    if not os.path.exists(json_path):\n",
    "        logger.error(f\"JSON file does not exist: {json_path}\")\n",
    "        print(f\"Error: JSON file does not exist: {json_path}\")\n",
    "        return None\n",
    "\n",
    "    # Load the annotations\n",
    "    try:\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            annotations = json.load(json_file)\n",
    "            logger.info(f\"Loaded {len(annotations)} annotations from {json_path}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error decoding JSON file: {e}\")\n",
    "        print(f\"Error decoding JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Randomly select a subset of the data\n",
    "    random.seed(seed)\n",
    "    if len(annotations) > max_images:\n",
    "        annotations_subset = random.sample(annotations, max_images)\n",
    "        logger.info(f\"Selected {max_images} random images for evaluation.\")\n",
    "    else:\n",
    "        annotations_subset = annotations\n",
    "        logger.warning(f\"Dataset contains fewer than {max_images} images. Using all available images ({len(annotations_subset)}).\")\n",
    "\n",
    "    # Initialize the DNN-based face detector\n",
    "    if not os.path.exists(prototxt_path):\n",
    "        logger.error(f\"Prototxt file does not exist: {prototxt_path}\")\n",
    "        print(f\"Error: Prototxt file does not exist: {prototxt_path}\")\n",
    "        return None\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        logger.error(f\"Caffe model file does not exist: {model_path}\")\n",
    "        print(f\"Error: Caffe model file does not exist: {model_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n",
    "        logger.info(\"Initialized OpenCV DNN-based face detector.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading DNN model: {e}\")\n",
    "        print(f\"Error loading DNN model: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Measure prediction times\n",
    "    prediction_times = []\n",
    "\n",
    "    for image_annotation in tqdm(annotations_subset, desc=\"Measuring Prediction Times\"):\n",
    "        # Load the image\n",
    "        image_path = image_annotation.get('image_path', '')\n",
    "        if not image_path:\n",
    "            logger.warning(\"No image path provided for an annotation. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            logger.error(f\"Image file does not exist: {image_path}. Skipping.\")\n",
    "            print(f\"Error: Image file does not exist: {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            logger.error(f\"Error: Could not load image at {image_path}. Skipping.\")\n",
    "            print(f\"Error: Could not load image at {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        (h, w) = image.shape[:2]\n",
    "\n",
    "        # Prepare the blob for DNN\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0,\n",
    "                                     (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Set the blob as input to the network and perform a forward-pass to get detections\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "\n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate prediction time\n",
    "        prediction_time = end_time - start_time\n",
    "        prediction_times.append(prediction_time)\n",
    "\n",
    "    # Calculate the average prediction time\n",
    "    if prediction_times:\n",
    "        average_time = sum(prediction_times) / len(prediction_times)\n",
    "        logger.info(f\"Average prediction time (OpenCV DNN): {average_time:.4f} seconds per image\")\n",
    "        print(f\"Average prediction time (OpenCV DNN): {average_time:.4f} seconds per image\")\n",
    "        return average_time\n",
    "    else:\n",
    "        logger.warning(\"No valid images were processed.\")\n",
    "        print(\"No valid images were processed.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    json_path = \"Data/WIDER FACE Validation Set/wider_face_val_annotations.json\"\n",
    "    prototxt_path = r\"C:\\Users\\sotir\\Project\\models\\OpenCV\\deploy.prototxt\"  # Update with your path\n",
    "    model_path = r\"C:\\Users\\sotir\\Project\\models\\OpenCV\\res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "    calculate_average_prediction_time_opencv_dnn(json_path, prototxt_path, model_path, confidence_threshold=0.5, max_images=100, seed=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349b05d-a3c7-43ce-b900-46374a9f6853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yolov5_env)",
   "language": "python",
   "name": "yolov5_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
